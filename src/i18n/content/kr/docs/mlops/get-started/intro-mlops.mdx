---
title: 모델 성능 모니터링(MLOps) 소개
metaDescription: Use New Relic ML model performance monitoring to monitor and observe the performance of your machine learning models.
translationType: machine
---

기계 학습 작업은 품질을 높이고 관리 프로세스를 단순화하며 대규모 프로덕션 환경에서 기계 학습 모델의 배포를 자동화하도록 설계된 일련의 사례입니다.

더 많은 회사가 인공 지능 및 기계 학습에 투자함에 따라 기계 학습 모델을 개발하는 데이터 과학 팀과 해당 모델을 지원하는 애플리케이션을 운영하는 DevOps 팀 사이에 이해의 격차가 있습니다. 현재 기업의 15%만이 전체 활동을 포괄하기 위해 AI를 배치합니다. 배포, 모니터링, 관리 및 거버넌스의 문제로 인해 프로덕션에서 기계 학습 모델의 75%가 전혀 사용되지 않는 것은 도움이 되지 않습니다. 궁극적으로 이는 모델 작업을 하는 엔지니어와 데이터 과학자에게 막대한 시간 낭비, 회사가 투자한 막대한 순 손실, 기계 학습 모델이 정량화 가능한 성장을 가능하게 할 때 전반적인 신뢰 부족으로 이어집니다.

당사의 모델 성능 모니터링은 데이터 과학자와 MLOps 실무자에게 프로덕션에서 모델의 동작과 효율성을 모니터링하여 기계 학습 애플리케이션의 성능에 대한 가시성을 제공합니다. 이를 통해 데이터 팀은 지속적인 개발, 테스트 및 운영 모니터링 프로세스를 생성하는 DevOps 팀과 직접 협업할 수 있습니다.

아직 하지 않았다면 아래에서 무료 New Relic 계정을 만들어 오늘 데이터 모니터링을 시작하십시오.

<InlineSignup/>

## 기계 학습 모델을 모니터링하는 방법 [#use-mlops]

New Relic의 [응용 인텔리전스](/docs/alerts-applied-intelligence/new-relic-alerts/get-started/introduction-applied-intelligence/) 내에서 모델 성능 모니터링을 사용하려면 몇 가지 옵션이 있습니다.

1. **BYOD(Bring Your Own Data):** New Relic에서 권장하는 접근 방식입니다. 당사의 ML 모델 성능 모니터링은 ML 모델이 프로덕션에서 작동하는 방식에 대한 심층적인 관찰 기능을 제공합니다. BYOD는 모든 환경(Python 스크립트, 컨테이너, Lambda 함수, SageMaker 등)에서 사용할 수 있으며 모든 기계 학습 프레임워크(Scikit-learn, Keras, Pytorch, Tensorflow, Jax 등)와 쉽게 통합될 수 있습니다. BYOD를 사용하면 자신의 ML 모델 원격 측정을 New Relic으로 가져와 ML 모델 데이터에서 가치를 얻을 수 있습니다. 단 몇 분 만에 모니터링하려는 다른 사용자 지정 메트릭과 함께 기능 분포, 통계 데이터 및 예측 분포를 얻을 수 있습니다. [문서에서 BYOD에 대해](/docs/alerts-applied-intelligence/mlops/bring-your-own/mlops-byo) 자세히 알아보세요.

2. **통합:** New Relic은 또한 Amazon SageMaker와 파트너 관계를 맺어 SageMaker에서 New Relic으로의 성능 메트릭 보기를 제공하고 ML 엔지니어 및 데이터 과학 팀의 관찰 가능성에 대한 액세스를 확장합니다. [Amazon SageMaker 통합](/docs/mlops/integrations/aws-sagemaker-mlops-integration/) 에 대해 자세히 알아보세요.

3. **파트너십:** New Relic은 특정 사용 사례 및 모니터링 기능을 제공하는 7개의 MLOps 벤더와 파트너십을 맺었습니다. 파트너는 모델에 대한 즉각적인 가시성을 제공하는 즉시 사용 가능한 대시보드를 제공하는 선별된 성능 대시보드 및 기타 관찰 가능성 도구에 액세스할 수 있는 좋은 방법입니다.

   우리는 현재 다음과 협력하고 있습니다:

   * [데이터로봇(알고리즘)](/docs/alerts-applied-intelligence/mlops/integrations/datarobot-mlops-integration/)
   * [아포리아](/docs/alerts-applied-intelligence/mlops/integrations/aporia-mlops-integration/)
   * [혜성](/docs/alerts-applied-intelligence/mlops/integrations/comet-mlops-integration/)
   * [DagsHub](/docs/alerts-applied-intelligence/mlops/integrations/dagshub-mlops-integration/)
   * [모나](/docs/alerts-applied-intelligence/mlops/integrations/mona-mlops-integration/)
   * [슈퍼와이즈](/docs/alerts-applied-intelligence/mlops/integrations/superwise-mlops-integration/)
   * [트루에라](/docs/alerts-applied-intelligence/mlops/integrations/truera-mlops-integration/)

이러한 옵션 중 하나를 사용하여 몇 분 안에 기계 학습 모델 성능 측정을 시작하려면 [모델 성능 모니터링 빠른 시작](https://newrelic.com/instant-observability/?category=machine-learning-ops) 을 확인하세요.

## OpenAI GPT 앱을 모니터링하는 방법 [#monitor-openai-gtp]

[GPT 시리즈 애플리케이션 통합을](/docs/mlops/integrations/openai-integration/)사용하면 OpenAI 완료 쿼리를 모니터링하고 요청에 대한 New Relic 사용자 지정 가능한 대시보드에 유용한 통계를 기록할 수 있습니다. 두 줄의 코드만 추가하면 비용, 응답 시간 및 샘플 입력/출력과 같은 주요 성능 메트릭에 액세스할 수 있습니다. 완전히 사용자 정의 가능한 대시보드를 통해 사용자는 총 요청, 평균 토큰/요청 및 모델 이름을 추적할 수 있습니다. [New Relic OpenAI Quickstart를](https://newrelic.com/instant-observability/openai)방문하여 자세히 알아보거나 통합을 설치하십시오.